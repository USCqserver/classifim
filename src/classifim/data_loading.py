import numpy as np
import torch
from classifim.utils import packbits

class TwelveSitesDataLoader():
    def __init__(self, data, batch_size):
        self.lambda0 = torch.from_numpy(data["lambda0"])
        self.dlambda = torch.from_numpy(data["dlambda"])
        self.zs = torch.from_numpy(data["zs"].transpose(0, 2, 1))
        self.label = torch.from_numpy(data["label"])
        self.batch_size = batch_size
        self.num_samples = self.lambda0.shape[0]
        self.pos = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.pos >= self.num_samples:
            self.pos = 0
            raise StopIteration()
        i0 = self.pos
        i1 = min(self.pos + self.batch_size, self.num_samples)
        self.pos = i1
        return (
            self.lambda0[i0:i1], self.dlambda[i0:i1], self.zs[i0:i1],
            self.label[i0:i1])

class TwelveSitesDataLoader2():
    def __init__(self, data, batch_size, device):
        """
        Args:
            data: a dictionary with the following keys:
            - lambdas: (num_orig_samples, num_lambdas) array of Hamiltonian parameters.
            - zs: int32 np.ndarray of shape (num_orig_samples, )
            - ii0,ii1: int32 np.ndarrays of shape (num_samples, )
        """
        self.lambdas = (torch.from_numpy(data["lambdas"].astype(np.float32))
                        .to(device=device))
        self.packed_zs = (torch.from_numpy(data["zs"].astype(np.int32))
                          .to(device=device))
        self.ii0 = (torch.from_numpy(data["ii0"].astype(np.int64))
                    .to(device=device))
        self.ii1 = (torch.from_numpy(data["ii1"].astype(np.int64))
                    .to(device=device))
        self.batch_size = batch_size
        self.device = device
        self.ii0_size = self.ii0.shape[0]
        self.num_samples = self.ii0_size * 2
        self.pos = 0

    @staticmethod
    def prepare_dataset(data, classifim_data, n_sites):
        """
        Prepare dataset in the format supported by this class.

        Args:
            data: a dictionary with the following keys:
                - lambdas: (num_orig_samples, num_lambdas)
                    array of Hamiltonian parameters.
                - zs: (num_orig_samples, 2, n_sites) array of spins.
            classifim_data: a dictionary with the following keys:
                - ii0, ii1: int32 np.ndarrays of shape (num_samples / 2, )
                This can be generated by get_classifim_train_dataset.
        """
        zs_01 = (1 - data["zs"].reshape(-1, 2 * n_sites)) / 2
        return {
            "lambdas": data["lambdas"],
            "zs": packbits(zs_01.astype(np.int32)),
            "ii0": classifim_data["ii0"],
            "ii1": classifim_data["ii1"]}

    def __iter__(self):
        self.pos = 0
        return self

    def _unpack_zs(self, packed_zs):
        # If we would unpack on CPU using numpy, we could just do
        # zs = ((packed_zs[..., np.newaxis] & (1 << np.arange(2 * n_sites)))) > 0
        # zs = (1 - 2 * zs).reshape((i1 - i0, 2, n_sites))
        # zs = zs.transpose(0, 2, 1)
        # However, we want to unpack on GPU, so we do the following:
        num_samples = packed_zs.shape[0]
        mask = 1 << torch.arange(
                2 * self.n_sites, device=self.device, dtype=torch.int32)
        zs = (packed_zs[:, None] & mask) > 0
        zs = (1 - 2 * zs.to(torch.float32)).view((num_samples, 2, self.n_sites))
        zs = zs.transpose(1, 2)
        return zs

    def _half_samples(self, i0, i1, label_value):
        ii0 = self.ii0[i0:i1]
        ii1 = self.ii1[i0:i1]
        lambda_m = self.lambdas[ii0]
        lambda_p = self.lambdas[ii1]
        lambda0 = (lambda_p + lambda_m) / 2
        dlambda = lambda_p - lambda_m
        packed_zs = self.packed_zs[ii0 if label_value == 0 else ii1]
        zs = self._unpack_zs(packed_zs)
        label = torch.full(
                (i1 - i0,), label_value,
                dtype=torch.float32, device=self.device)
        return (lambda0, dlambda, zs, label)

    def _retrieve_samples(self, i0, i1):
        if i1 <= self.ii0_size:
            return self._half_samples(i0, i1, 0)
        elif i0 >= self.ii0_size:
            return self._half_samples(i0 - self.ii0_size, i1 - self.ii0_size, 1)
        else:
            return tuple(
                torch.cat([v0, v1], dim=0)
                for v0, v1 in zip(
                    self._half_samples(i0, self.ii0_size, 0),
                    self._half_samples(0, i1 - self.ii0_size, 1)))

    def __next__(self):
        if self.pos >= self.num_samples:
            self.pos = 0
            raise StopIteration()
        i0 = self.pos
        i1 = min(self.pos + self.batch_size, self.num_samples)
        self.pos = i1
        return self._retrieve_samples(i0, i1)

class TwelveSitesDataLoader3():
    def __init__(self, data, batch_size, device, shuffle=True, n_sites=None):
        """
        Args:
            data: a dictionary with the following keys:
            - lambdas: (num_orig_samples, num_lambdas) array of Hamiltonian parameters.
            - zs: int32 np.ndarray of shape (num_orig_samples, )
            - ii0,ii1: int32 np.ndarrays of shape (num_samples, )
        """
        self.lambdas = (torch.from_numpy(data["lambdas"].astype(np.float32))
                        .to(device=device))
        self.packed_zs = (torch.from_numpy(data["zs"].astype(np.int32))
                          .to(device=device))
        ii0 = (torch.from_numpy(data["ii0"].astype(np.int64))
                    .to(device=device))
        ii1 = (torch.from_numpy(data["ii1"].astype(np.int64))
                    .to(device=device))
        self.ii0 = torch.cat([ii0, ii1], dim=0)
        self.ii1 = torch.cat([ii1, ii0], dim=0)
        if n_sites is None:
            n_sites = data.get("n_sites", None)
        assert n_sites is not None, "n_sites must be specified"
        self.n_sites = n_sites
        self.batch_size = batch_size
        self.device = device
        self.shuffle = shuffle
        self.num_samples = self.ii0.size(0)
        self.pos = 0

    @staticmethod
    def prepare_dataset(data, classifim_data, n_sites):
        """
        Prepare dataset in the format supported by this class.

        Note: zs are taken from data and converted to packed_zs.
        Input zs have shape (num_orig_samples, 2, n_sites), i.e. have format
        [[Z_{B0}, Z_{B1}, ..., Z_{B11}], [Z_{T0}, ..., Z_{T11}]]
        Output packed_zs have shape (num_orig_samples, ).
        Output packed_zs format: 0bT[11]T[10]...T[0]B[11]...B[0]

        Args:
            data: a dictionary with the following keys:
                - lambdas: (num_orig_samples, num_lambdas)
                    array of Hamiltonian parameters.
                - zs: (num_orig_samples, 2, n_sites) array of spins.
            classifim_data: a dictionary with the following keys:
                - ii0, ii1: int32 np.ndarrays of shape (num_samples / 2, )
                This can be generated by get_classifim_train_dataset.
        """
        zs_01 = (1 - data["zs"].reshape(-1, 2 * n_sites)) / 2
        return {
            "lambdas": data["lambdas"],
            "zs": packbits(zs_01.astype(np.int32)),
            "ii0": classifim_data["ii0"],
            "ii1": classifim_data["ii1"]}

    def _shuffle(self):
        rand_perm = torch.randperm(self.num_samples, device=self.device)
        self.ii0 = self.ii0[rand_perm]
        self.ii1 = self.ii1[rand_perm]

    def __iter__(self):
        if self.shuffle:
            self._shuffle()
        self.pos = 0
        return self

    def _unpack_zs(self, packed_zs):
        """
        Input packed_zs format: 0bT[11]T[10]...T[0]B[11]...B[0]
        Output zs shape: (num_samples, n_sites, 2)
        format: [[Z_{B0}, Z_{T0}], [Z_{B1}, Z_{T1}], ..., [Z_{B11}, Z_{T11}]]
        """
        # If we would unpack on CPU using numpy, we could just do
        # zs = ((packed_zs[..., np.newaxis] & (1 << np.arange(2 * n_sites)))) > 0
        # zs = (1 - 2 * zs).reshape((i1 - i0, 2, n_sites))
        # zs = zs.transpose(0, 2, 1)
        # However, we want to unpack on GPU, so we do the following:
        num_samples = packed_zs.shape[0]
        mask = 1 << torch.arange(2 * self.n_sites, device=self.device, dtype=torch.int32)
        zs = (packed_zs[:, None] & mask) > 0
        zs = (1 - 2 * zs.to(torch.float32)).view((num_samples, 2, self.n_sites))
        zs = zs.transpose(1, 2)
        return zs

    def _retrieve_samples(self, i0, i1):
        ii0 = self.ii0[i0:i1]
        ii1 = self.ii1[i0:i1]
        lambda_m = self.lambdas[ii0]
        lambda_p = self.lambdas[ii1]
        lambda0 = (lambda_p + lambda_m) / 2
        dlambda = lambda_p - lambda_m
        packed_zs = self.packed_zs[ii1]
        zs = self._unpack_zs(packed_zs)
        label = torch.ones((i1 - i0,), dtype=torch.float32, device=self.device)
        return (lambda0, dlambda, zs, label)

    def __next__(self):
        if self.pos >= self.num_samples:
            self.pos = 0
            raise StopIteration()
        i0 = self.pos
        i1 = min(self.pos + self.batch_size, self.num_samples)
        self.pos = i1
        return self._retrieve_samples(i0, i1)

class TwelveSitesDataLoader4():
    """
    Similar to TwelveSitesDataLoader3, but keeps the pairs
    with the same lambda0 and dlambda**2 in the same batch.
    """
    def __init__(self, data, batch_size, device, shuffle=True, n_sites=None):
        """
        Args:
            data: a dictionary with the following keys:
            - lambdas: (num_orig_samples, num_lambdas) array of Hamiltonian parameters.
            - zs: int32 np.ndarray of shape (num_orig_samples, )
            - ii0,ii1: int32 np.ndarrays of shape (num_samples, )
        """
        self.lambdas = (torch.from_numpy(data["lambdas"].astype(np.float32))
                        .to(device=device))
        self.packed_zs = (torch.from_numpy(data["zs"].astype(np.int32))
                          .to(device=device))
        self.ii0 = (torch.from_numpy(data["ii0"].astype(np.int64))
                    .to(device=device))
        self.ii1 = (torch.from_numpy(data["ii1"].astype(np.int64))
                    .to(device=device))
        assert batch_size % 2 == 0, f"batch_size must be even, not {batch_size}"
        if n_sites is None:
            n_sites = data.get("n_sites", None)
        assert n_sites is not None, "n_sites must be specified"
        self.n_sites = n_sites
        self.pair_batch_size = batch_size // 2
        self.device = device
        self.shuffle = shuffle
        self.num_sample_pairs = self.ii0.size(0)
        self.num_samples = self.num_sample_pairs * 2
        self.pos = 0

    @staticmethod
    def prepare_dataset(data, classifim_data, n_sites):
        """
        Prepare dataset in the format supported by this class.

        Args:
            data: a dictionary with the following keys:
                - lambdas: (num_orig_samples, num_lambdas)
                    array of Hamiltonian parameters.
                - zs: (num_orig_samples, 2, n_sites) array of spins.
            classifim_data: a dictionary with the following keys:
                - ii0, ii1: int32 np.ndarrays of shape (num_samples / 2, )
                This can be generated by get_classifim_train_dataset.
        """
        zs_01 = (1 - data["zs"].reshape(-1, n_sites * 2)) / 2
        return {
            "lambdas": data["lambdas"],
            "zs": packbits(zs_01.astype(np.int32)),
            "ii0": classifim_data["ii0"],
            "ii1": classifim_data["ii1"]}

    def _shuffle(self):
        rand_perm = torch.randperm(self.num_sample_pairs, device=self.device)
        self.ii0 = self.ii0[rand_perm]
        self.ii1 = self.ii1[rand_perm]

    def __iter__(self):
        if self.shuffle:
            self._shuffle()
        self.pos = 0
        return self

    def _unpack_zs(self, packed_zs):
        # If we would unpack on CPU using numpy, we could just do
        # zs = ((packed_zs[..., np.newaxis] & (1 << np.arange(2 * n_sites)))) > 0
        # zs = (1 - 2 * zs).reshape((i1 - i0, 2, n_sites))
        # zs = zs.transpose(0, 2, 1)
        # However, we want to unpack on GPU, so we do the following:
        num_samples = packed_zs.shape[0]
        mask = 1 << torch.arange(
                2 * self.n_sites, device=self.device, dtype=torch.int32)
        zs = (packed_zs[:, None] & mask) > 0
        zs = (1 - 2 * zs.to(torch.float32)).view((num_samples, 2, self.n_sites))
        zs = zs.transpose(1, 2)
        return zs

    def _retrieve_samples(self, i0, i1):
        """
        Retrieve 2 * (i1 - i0) samples from the dataset.
        """
        ii0 = self.ii0[i0:i1]
        ii1 = self.ii1[i0:i1]
        lambda_m = self.lambdas[ii0]
        lambda_p = self.lambdas[ii1]
        lambda0 = (lambda_p + lambda_m) / 2
        dlambda = lambda_p - lambda_m
        packed_zs_m = self.packed_zs[ii0]
        packed_zs_p = self.packed_zs[ii1]
        zs_m = self._unpack_zs(packed_zs_m)
        zs_p = self._unpack_zs(packed_zs_p)
        label = torch.ones((2 * (i1 - i0),), dtype=torch.float32, device=self.device)
        return (
            torch.cat((lambda0, lambda0), dim=0),
            torch.cat((-dlambda, dlambda), dim=0),
            torch.cat((zs_m, zs_p), dim=0),
            label)

    def __next__(self):
        if self.pos >= self.num_sample_pairs:
            self.pos = 0
            raise StopIteration()
        i0 = self.pos
        i1 = min(self.pos + self.pair_batch_size, self.num_sample_pairs)
        self.pos = i1
        return self._retrieve_samples(i0, i1)

def enumerate_distinct_rows(lambda_array):
    """
    Given a 2D array of shape (num_samples, num_lambdas), assign a unique
    integer category to each distinct row.
    """
    # Sort the array lexicographically and get the indices
    sorted_indices = np.lexsort(lambda_array.T)
    sorted_lambda = lambda_array[sorted_indices]

    # Find the unique rows and their indices
    unique_rows_mask = np.ones(len(sorted_lambda), dtype=bool)
    unique_rows_mask[1:] = np.any(sorted_lambda[1:] != sorted_lambda[:-1], axis=1)

    # Assign category numbers to unique rows
    categories = np.cumsum(unique_rows_mask) - 1

    # Create an output array with the shape (num_samples,) and restore the original order
    category = np.empty(len(lambda_array), dtype=int)
    category[sorted_indices] = categories

    return category

class TwelveSitesDataLoader5:
    """
    Similar to TwelveSitesDataLoader4, but
    - does not require 'ii0' and 'ii1'. Instead, it
    has a method `reshuffle` which recreates `ii0` and `ii1`.
    """
    def __init__(self, data, batch_size, device, n_sites):
        """
        Args:
            data: a dictionary with the following keys:
            - lambdas: (num_orig_samples, num_lambdas) array of Hamiltonian parameters.
              Range: [-1, 1).
            - zs: int32 np.ndarray of shape (num_orig_samples, )
            batch_size: number of samples per batch (i.e. #samples returned by
                __next__). batch_size must be even.
            device: torch.device
            n_sites: number of sites in each row of the lattice lattice.
        """
        self.lambdas = (torch.from_numpy(data["lambdas"].astype(np.float32))
                        .to(device=device))
        self.packed_zs = (torch.from_numpy(data["zs"].astype(np.int32))
                          .to(device=device))
        category = enumerate_distinct_rows(data["lambdas"])
        self.category = torch.from_numpy(category).to(device=device)
        assert batch_size % 2 == 0, f"batch_size must be even, not {batch_size}"
        self.pair_batch_size = batch_size // 2
        self.device = device
        self.max_sample_pairs = self.lambdas.shape[0]
        self.num_sample_pairs = None
        self.reshuffle()
        self.pos = 0
        assert n_sites is not None
        self.n_sites = n_sites

    def reshuffle(self):
        ii0 = torch.randperm(self.max_sample_pairs, device=self.device)
        ii1 = torch.randperm(self.max_sample_pairs, device=self.device)
        i_valid = (self.category[ii0] != self.category[ii1]).nonzero().view(-1)
        self.ii0 = ii0[i_valid]
        self.ii1 = ii1[i_valid]
        self.num_sample_pairs = len(i_valid)

    def __iter__(self):
        self.pos = 0
        return self

    def _unpack_zs(self, packed_zs):
        # If we would unpack on CPU using numpy, we could just do
        # zs = ((packed_zs[..., np.newaxis] & (1 << np.arange(2 * n_sites)))) > 0
        # zs = (1 - 2 * zs).reshape((i1 - i0, 2, n_sites))
        # zs = zs.transpose(0, 2, 1)
        # However, we want to unpack on GPU, so we do the following:
        num_samples = packed_zs.shape[0]
        mask = 1 << torch.arange(2 * self.n_sites, device=self.device, dtype=torch.int32)
        zs = (packed_zs[:, None] & mask) > 0
        zs = (1 - 2 * zs.to(torch.float32)).view((num_samples, 2, self.n_sites))
        zs = zs.transpose(1, 2)
        return zs

    def _retrieve_samples(self, i0, i1):
        """
        Retrieve 2 * (i1 - i0) samples from the dataset.
        """
        ii0 = self.ii0[i0:i1]
        ii1 = self.ii1[i0:i1]
        lambda_m = self.lambdas[ii0]
        lambda_p = self.lambdas[ii1]
        lambda0 = (lambda_p + lambda_m) / 2
        dlambda = lambda_p - lambda_m
        packed_zs_m = self.packed_zs[ii0]
        packed_zs_p = self.packed_zs[ii1]
        zs_m = self._unpack_zs(packed_zs_m)
        zs_p = self._unpack_zs(packed_zs_p)
        label = torch.ones((2 * (i1 - i0),), dtype=torch.float32, device=self.device)
        return (
            torch.cat((lambda0, lambda0), dim=0),
            torch.cat((-dlambda, dlambda), dim=0),
            torch.cat((zs_m, zs_p), dim=0),
            label)

    def __next__(self):
        if self.pos >= self.num_sample_pairs:
            self.pos = 0
            raise StopIteration()
        i0 = self.pos
        i1 = min(self.pos + self.pair_batch_size, self.num_sample_pairs)
        self.pos = i1
        return self._retrieve_samples(i0, i1)

class InMemoryLoader:
    """
    Keeps the entire dataset in memory.
    """
    def __init__(self, data, batch_size, device, subsample=None, **kwargs):
        """
        Args:
            data: a dictionary with the following keys:
            - scaled_lambdas: (num_orig_samples, num_lambdas) array of
                Hamiltonian parameters. Range: [-1, 1).
            - zs-related data (consumed by _init_zs in derived classes).
            batch_size: number of samples per batch (i.e. #samples returned by
                __next__). batch_size must be even.
            device: torch.device
            subsample: if not None, use random choice of that size from the
                dataset (the choice is different for each epoch).
            **kwargs: additional arguments passed to _init_zs in derived
                classes.
        """
        self.lambdas = (
            torch.from_numpy(data["scaled_lambdas"].astype(np.float32))
            .to(device=device))
        category = enumerate_distinct_rows(data["scaled_lambdas"])
        self.category = torch.from_numpy(category).to(device=device)
        assert batch_size % 2 == 0, f"batch_size must be even, not {batch_size}"
        self.pair_batch_size = batch_size // 2
        self.device = device
        self.subsample = subsample
        self.max_sample_pairs = self.lambdas.shape[0]
        self.num_sample_pairs = None
        self._init_zs(data, device, **kwargs)
        self.reshuffle()
        self.pos = 0

    def reshuffle(self):
        """
        Reshuffle the dataset before the next epoch.
        """
        ii0 = torch.randperm(self.max_sample_pairs, device=self.device)
        ii1 = torch.randperm(self.max_sample_pairs, device=self.device)
        if self.subsample is not None:
            ii0 = ii0[:self.subsample]
            ii1 = ii1[:self.subsample]
        i_valid = (self.category[ii0] != self.category[ii1]).nonzero().view(-1)
        self.ii0 = ii0[i_valid]
        self.ii1 = ii1[i_valid]
        self.num_sample_pairs = len(i_valid)

    def __iter__(self):
        self.pos = 0
        return self

    def _init_zs(self, data, device, **kwargs):
        raise NotImplementedError()

    def _retrieve_zs(self, ii):
        """
        Retrieve zs for indices in ii.

        Args:
            ii: 1D tensor of indices.

        Returns: tuple, each element of which is a tensor of shape
            (len(ii), ...)
        """
        raise NotImplementedError()

    def _retrieve_samples(self, i0, i1):
        """
        Retrieve 2 * (i1 - i0) samples from the dataset.
        """
        ii0 = self.ii0[i0:i1]
        ii1 = self.ii1[i0:i1]
        lambda_m = self.lambdas[ii0]
        lambda_p = self.lambdas[ii1]
        lambda0 = (lambda_p + lambda_m) / 2
        dlambda = lambda_p - lambda_m
        zs_m = self._retrieve_zs(ii0)
        zs_p = self._retrieve_zs(ii1)
        label = torch.ones(
            (2 * (i1 - i0),), dtype=torch.float32, device=self.device)
        return (
            torch.cat((lambda0, lambda0), dim=0),
            torch.cat((-dlambda, dlambda), dim=0),
            *(torch.cat((zsi_m, zsi_p), dim=0)
                for zsi_m, zsi_p in zip(zs_m, zs_p)),
            label)

    def __next__(self):
        if self.pos >= self.num_sample_pairs:
            self.pos = 0
            raise StopIteration()
        i0 = self.pos
        i1 = min(self.pos + self.pair_batch_size, self.num_sample_pairs)
        self.pos = i1
        return self._retrieve_samples(i0, i1)

class FimLoader:
    """
    Base class for DataLoaders for FIM estimation.

    This requires the base methods to implement _init_zs and _retrieve_zs:
    the same as InMemoryLoader, allowing one to use the same mixin for both.
    """
    def __init__(self, data, batch_size, device, **kwargs):
        self.num_samples = data["scaled_lambdas"].shape[0]
        self.batch_size = batch_size
        self.device = device
        self._init_zs(data, device, **kwargs)

    def _init_zs(self, data, device, **kwargs):
        raise NotImplementedError()

    def _retrieve_zs(self, ii):
        """
        Retrieve zs for indices in ii.

        Args:
            ii: slice of indices

        Returns: tuple, each element of which is a tensor of shape
            (len(ii), ...)
        """
        raise NotImplementedError()

    def __iter__(self):
        self.pos = 0
        return self

    def __next__(self):
        if self.pos >= self.num_samples:
            self.pos = 0
            raise StopIteration()
        batch_start = self.pos
        self.pos = min(self.pos + self.batch_size, self.num_samples)
        batch_end = self.pos
        cur_slice = slice(batch_start, batch_end)
        return (cur_slice, self._retrieve_zs(cur_slice))

